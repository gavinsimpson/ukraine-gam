<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Generalized Additive Models in R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Gavin Simpson" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: inverse, middle, left, my-title-slide, title-slide

.title[
# Generalized Additive Models in R
]
.author[
### Gavin Simpson
]
.date[
### Workshops for Ukraine Â· April 13, 2023
]

---




# Logistics

* HTML Slide deck [bit.ly/ukraine-gam-slides](https://bit.ly/ukraine-gam-slides) &amp;copy; Simpson (2023) [![Creative Commons Licence](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)
* RMarkdown [bit.ly/ukraine-gam-repo](https://bit.ly/ukraine-gam-repo)
* DOI: [10.5281/zenodo.7825960](https://doi.org/10.5281/zenodo.7825960)

Open RStudio &amp; run


```r
usethis::use_course("https://bit.ly/ukraine-gam")
```

---
class: inverse middle center subsection

# Motivating example

---

# HadCRUT4 time series

![](index_files/figure-html/hadcrut-temp-example-1.svg)&lt;!-- --&gt;

???

Hadley Centre NH temperature record ensemble

How would you model the trend in these data?

---

# Linear Models

`$$y_i \sim \mathcal{N}(\mu_i, \sigma^2)$$`

`$$\mu_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_j x_{ji}$$`

Assumptions

1. linear effects of covariates are good approximation of the true effects
2. conditional on the values of covariates, `\(y_i | \mathbf{X} \sim \mathcal{N}(0, \sigma^2)\)`
3. this implies all observations have the same *variance*
4. `\(y_i | \mathbf{X}\)` are *independent*

An **additive** model address the first of these

---
class: inverse center middle subsection

# Why bother with anything more complex?

---

# Is this linear?

![](index_files/figure-html/hadcrut-temp-example-1.svg)&lt;!-- --&gt;

---

# Polynomials perhaps&amp;hellip;

![](index_files/figure-html/hadcrut-temp-polynomial-1.svg)&lt;!-- --&gt;

---

# Polynomials perhaps&amp;hellip;

We can keep on adding ever more powers of `\(\boldsymbol{x}\)` to the model &amp;mdash; model selection problem

**Runge phenomenon** &amp;mdash; oscillations at the edges of an interval &amp;mdash; means simply moving to higher-order polynomials doesn't always improve accuracy

---
class: inverse middle center subsection

# GAMs offer a solution

---
class: inverse middle center big-subsection

# Climate example

---

# HadCRUT data set


```r
## Load Data
library("readr")
library("dplyr")
URL &lt;-  "https://bit.ly/hadcrutv4"
gtemp &lt;- read_table(URL, col_types = "nnnnnnnnnnnn", col_names = FALSE) %&gt;%
    select(num_range("X", 1:2)) %&gt;% setNames(nm = c("Year", "Temperature"))
```

[File format](https://www.metoffice.gov.uk/hadobs/hadcrut4/data/current/series_format.html)

---

# HadCRUT data set


```r
gtemp
```

```
## # A tibble: 172 Ã— 2
##     Year Temperature
##    &lt;dbl&gt;       &lt;dbl&gt;
##  1  1850      -0.336
##  2  1851      -0.159
##  3  1852      -0.107
##  4  1853      -0.177
##  5  1854      -0.071
##  6  1855      -0.19 
##  7  1856      -0.378
##  8  1857      -0.405
##  9  1858      -0.4  
## 10  1859      -0.215
## # â„¹ 162 more rows
```

---

# Fitting a GAM


```r
library("mgcv")
m &lt;- gam(Temperature ~ s(Year), data = gtemp, method = "REML")
summary(m)
```

.smaller[

```
## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## Temperature ~ s(Year)
## 
## Parametric coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) -0.020477   0.009731  -2.104   0.0369 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Approximate significance of smooth terms:
##           edf Ref.df     F p-value    
## s(Year) 7.837   8.65 145.1  &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-sq.(adj) =   0.88   Deviance explained = 88.6%
## -REML = -91.237  Scale est. = 0.016287  n = 172
```
]

---

# Fitted GAM

![](index_files/figure-html/hadcrtemp-plot-gam-1.svg)&lt;!-- --&gt;

---
class: inverse middle center big-subsection

# GAMs

---

# Generalized Additive Models

&lt;br /&gt;

![](resources/tradeoff-slider.png)

.references[Source: [GAMs in R by Noam Ross](https://noamross.github.io/gams-in-r-course/)]

???

GAMs are an intermediate-complexity model

* can learn from data without needing to be informed by the user
* remain interpretable because we can visualize the fitted features

---

# How is a GAM different?

In LM we model the mean of data as a sum of linear terms:

`$$y_i = \beta_0 +\sum_j \color{red}{ \beta_j x_{ji}} + \epsilon_i$$`

A GAM is a sum of _smooth functions_ or _smooths_

`$$y_i = \beta_0 + \sum_j \color{red}{f_j(x_{ji})} + \epsilon_i$$`

where `\(\epsilon_i \sim N(0, \sigma^2)\)`, `\(y_i | \mathbf{X} \sim \text{Normal}\)` (for now)

Call the above equation the **linear predictor** in both cases

---

# Fitting a GAM in R

```r
model &lt;- gam(y ~ s(x1) + s(x2) + te(x3, x4), # formula describing model
             data = my_data_frame,           # your data
             method = "REML",                # or "ML"
             family = gaussian)              # or something more exotic
```

`s()` terms are smooths of one or more variables

`te()` terms are the smooth equivalent of *main effects + interactions*

---

# How did `gam()` *know*?

![](index_files/figure-html/hadcrtemp-plot-gam-1.svg)&lt;!-- --&gt;

---
class: inverse
background-image: url('./resources/rob-potter-398564.jpg')
background-size: contain

# What magic is this?

.footnote[
&lt;a style="background-color:black;color:white;text-decoration:none;padding:4px 6px;font-family:-apple-system, BlinkMacSystemFont, &amp;quot;San Francisco&amp;quot;, &amp;quot;Helvetica Neue&amp;quot;, Helvetica, Ubuntu, Roboto, Noto, &amp;quot;Segoe UI&amp;quot;, Arial, sans-serif;font-size:12px;font-weight:bold;line-height:1.2;display:inline-block;border-radius:3px;" href="https://unsplash.com/@robpotter?utm_medium=referral&amp;amp;utm_campaign=photographer-credit&amp;amp;utm_content=creditBadge" target="_blank" rel="noopener noreferrer" title="Download free do whatever you want high-resolution photos from Rob Potter"&gt;&lt;span style="display:inline-block;padding:2px 3px;"&gt;&lt;svg xmlns="http://www.w3.org/2000/svg" style="height:12px;width:auto;position:relative;vertical-align:middle;top:-1px;fill:white;" viewBox="0 0 32 32"&gt;&lt;title&gt;&lt;/title&gt;&lt;path d="M20.8 18.1c0 2.7-2.2 4.8-4.8 4.8s-4.8-2.1-4.8-4.8c0-2.7 2.2-4.8 4.8-4.8 2.7.1 4.8 2.2 4.8 4.8zm11.2-7.4v14.9c0 2.3-1.9 4.3-4.3 4.3h-23.4c-2.4 0-4.3-1.9-4.3-4.3v-15c0-2.3 1.9-4.3 4.3-4.3h3.7l.8-2.3c.4-1.1 1.7-2 2.9-2h8.6c1.2 0 2.5.9 2.9 2l.8 2.4h3.7c2.4 0 4.3 1.9 4.3 4.3zm-8.6 7.5c0-4.1-3.3-7.5-7.5-7.5-4.1 0-7.5 3.4-7.5 7.5s3.3 7.5 7.5 7.5c4.2-.1 7.5-3.4 7.5-7.5z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;&lt;span style="display:inline-block;padding:2px 3px;"&gt;Rob Potter&lt;/span&gt;&lt;/a&gt;
]

---
class: inverse
background-image: url('resources/wiggly-things.png')
background-size: contain

???

---



# Wiggly things

.center[![](resources/spline-anim.gif)]

???

GAMs use splines to represent the non-linear relationships between covariates, here `x`, and the response variable on the `y` axis.

---

# Basis expansions

In the polynomial models we used a polynomial basis expansion of `\(\boldsymbol{x}\)`

* `\(\boldsymbol{x}^0 = \boldsymbol{1}\)` &amp;mdash; the model constant term
* `\(\boldsymbol{x}^1 = \boldsymbol{x}\)` &amp;mdash; linear term
* `\(\boldsymbol{x}^2\)`
* `\(\boldsymbol{x}^3\)`
* &amp;hellip;

---

# Splines

Splines are *functions* composed of simpler functions

Simpler functions are *basis functions* &amp; the set of basis functions is a *basis*

When we model using splines, each basis function `\(b_k\)` has a coefficient `\(\beta_k\)`

Resultant spline is a the sum of these weighted basis functions, evaluated at the values of `\(x\)`

`$$f(x) = \sum_{k = 1}^K \beta_k b_k(x)$$`

---

# Splines formed from basis functions

![](index_files/figure-html/basis-functions-1.svg)&lt;!-- --&gt;

???

Splines are built up from basis functions

Here I'm showing a cubic regression spline basis with 10 knots/functions

We weight each basis function to get a spline. Here all the basisi functions have the same weight so they would fit a horizontal line

---

# Weight basis functions &amp;#8680; spline



.center[![](resources/basis-fun-anim.gif)]

???

But if we choose different weights we get more wiggly spline

Each of the splines I showed you earlier are all generated from the same basis functions but using different weights

---

# How do GAMs learn from data?

![](index_files/figure-html/example-data-figure-1.svg)&lt;!-- --&gt;

???

How does this help us learn from data?

Here I'm showing a simulated data set, where the data are drawn from the orange functions, with noise. We want to learn the orange function from the data

---

# Maximise penalised log-likelihood &amp;#8680; &amp;beta;



.center[![](resources/gam-crs-animation.gif)]

???

Fitting a GAM involves finding the weights for the basis functions that produce a spline that fits the data best, subject to some constraints


---
class: inverse middle center subsection

# Avoid overfitting our sample

---
class: inverse center middle large-subsection

# How wiggly?

$$
\int_{\mathbb{R}} [f^{\prime\prime}]^2 dx = \boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}\boldsymbol{\beta}
$$

---
class: inverse center middle large-subsection

# Penalised fit

$$
\mathcal{L}_p(\boldsymbol{\beta}) = \mathcal{L}(\boldsymbol{\beta}) - \frac{1}{2} \lambda\boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}\boldsymbol{\beta}
$$

---

# Wiggliness

`$$\int_{\mathbb{R}} [f^{\prime\prime}]^2 dx = \boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}\boldsymbol{\beta} = \large{W}$$`

(Wiggliness is 100% the right mathy word)

We penalize wiggliness to avoid overfitting

---

# Making wiggliness matter

`\(W\)` measures **wiggliness**

(log) likelihood measures closeness to the data

We use a **smoothing parameter** `\(\lambda\)` to define the trade-off, to find
the spline coefficients `\(B_k\)` that maximize the **penalized** log-likelihood

`$$\mathcal{L}_p = \log(\text{Likelihood})  - \lambda W$$`

---

# HadCRUT4 time series

![](index_files/figure-html/hadcrut-temp-penalty-1.svg)&lt;!-- --&gt;

---

# Picking the right wiggliness

.pull-left[
Two ways to think about how to optimize `\(\lambda\)`:

* Predictive: Minimize out-of-sample error
* Bayesian:  Put priors on our basis coefficients
]

.pull-right[
Many methods: AIC, Mallow's `\(C_p\)`, GCV, ML, REML

* **Practically**, use **REML**, because of numerical stability
* Hence `gam(..., method = "REML")`
]

.center[
![Animation of derivatives](./resources/remlgcv.png)
]

---

# Maximum allowed wiggliness

We set **basis complexity** or "size" `\(k\)`

This is _maximum wigglyness_, can be thought of as number of small functions that make up a curve

Once smoothing is applied, curves have fewer **effective degrees of freedom (EDF)**

EDF &lt; `\(k\)`

---

# Maximum allowed wiggliness

`\(k\)` must be *large enough*, the `\(\lambda\)` penalty does the rest

*Large enough* &amp;mdash; space of functions representable by the basis includes the true function or a close approximation to the tru function

Bigger `\(k\)` increases computational cost

In **mgcv**, default `\(k\)` values are arbitrary &amp;mdash; after choosing the model terms, this is the key user choice

**Must be checked!** &amp;mdash; `gam.check()`


---

# GAM summary so far

1. GAMs give us a framework to model  flexible nonlinear relationships

2. Use little functions (**basis functions**) to make big functions (**smooths**)

3. Use a **penalty** to trade off wiggliness/generality 

4. Need to make sure your smooths are **wiggly enough**

---
class: inverse middle center subsection

# Ozone example

---
class: inverse middle center subsection

# A cornucopia of smooths

---

# A cornucopia of smooths

The type of smoother is controlled by the `bs` argument (think *basis*)

The default is a low-rank thin plate spline `bs = 'tp'`

Many others available

.small[
.row[
.col-6[

* Cubic splines `bs = 'cr'`
* P splines `bs = 'ps'`
* Cyclic splines `bs = 'cc'` or `bs = 'cp'`
* Adaptive splines `bs = 'ad'`
* Random effect `bs = 're'`
* Factor smooths `bs = 'fs'`
]
.col-6[

* Duchon splines `bs = 'ds'`
* Spline on the sphere `bs = 'sos'`
* MRFs `bs = 'mrf'`
* Soap-film smooth `bs = 'so'`
* Gaussian process `bs = 'gp'`
* Constrained factor smooth `bs = 'sz'`
]
]
]

---

# A bestiary of conditional distributions

A GAM is just a fancy GLM

Simon Wood &amp; colleagues (2016) have extended the *mgcv* methods to some non-exponential family distributions


.row[
.col-5[
* `binomial()`
* `poisson()`
* `Gamma()`
* `inverse.gaussian()`
* `nb()`
* `tw()`
* `mvn()`
]
.col-4[
* `multinom()`
* `betar()`
* `scat()`
* `gaulss()`
* `ziplss()`
* `twlss()`
* `cox.ph()`
]
.col-3[
* `gevlss()`
* `gamals()`
* `ocat()`
* `shash()`
* `gumbls()`
* `cnorm()`
* `ziP()`
]
]

---

# The whole process 1

![](index_files/figure-html/whole-basis-proces-1.svg)&lt;!-- --&gt;

---

# The whole process 2


```r
dat &lt;- data_sim("eg1", seed = 4)
m &lt;- gam(y ~ s(x0) + s(x1) + s(x2, bs = "bs") + s(x3),
         data = dat, method = "REML")
```


```r
draw(m) + plot_layout(ncol = 4)
```

![](index_files/figure-html/whole-basis-proces-2-model-draw-1.svg)&lt;!-- --&gt;

---

# The whole process 3

![](index_files/figure-html/whole-basis-proces-2-1.svg)&lt;!-- --&gt;

---
class: inverse middle center big-subsection

# Interactions

---

# Smooth interactions

Two ways to fit smooth interactions

1. Bivariate (or higher order) thin plate splines
    * `s(x, z, bs = 'tp')`
    * Isotropic; single smoothness parameter for the smooth
	* Sensitive to scales of `x` and `z`
2. Tensor product smooths
    * Separate marginal basis for each smooth, separate smoothness parameters
	* Invariant to scales of `x` and `z`
	* Use for interactions when variables are in different units
	* `te(x, z)`

---

# Smooth interactions



![](index_files/figure-html/draw-tprs-vs-tensor-product-truth-1.svg)&lt;!-- --&gt;

---

# Smooth interactions


```r
df
```

```
## # A tibble: 500 Ã— 3
##           y        x      z
##       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;
##  1  0.106   0.000670 0.804 
##  2  0.190   0.0191   0.759 
##  3  0.327   0.0435   0.957 
##  4  0.0271  0.0170   0.994 
##  5  0.441   0.0241   0.606 
##  6  0.00930 0.0300   0.0294
##  7  0.569   0.0247   0.336 
##  8  0.530   0.00931  0.278 
##  9  0.0855  0.0414   0.117 
## 10 -0.125   0.0334   0.0432
## # â„¹ 490 more rows
```

```r
m_tprs &lt;- gam(y ~ s(x, z), data = df, method = "REML")
m_te   &lt;- gam(y ~ te(x, z), data = df, method = "REML")
```

---

# Smooth interactions


```r
truth_plt + (draw(m_tprs) + coord_cartesian()) + draw(m_te) + plot_layout(ncol = 3)
```

&lt;img src="index_files/figure-html/draw-tprs-vs-tensor-product-1.svg" width="95%" /&gt;

---

# Smooth interactions


```r
layout(matrix(1:3, ncol = 3))
persp(xs, zs, truth)
vis.gam(m_tprs)
vis.gam(m_te)
layout(1)
```

![](index_files/figure-html/plot-tprs-vs-tensor-product-1.svg)&lt;!-- --&gt;

---

# Tensor product smooths

There are multiple ways to build tensor products in *mgcv*

1. `te(x, z)`
2. `t2(x, z)`
3. `s(x) + s(z) + ti(x, z)`

`te()` is the most general form but not usable in `gamm4::gamm4()` or *brms*

`t2()` is an alternative implementation that does work in `gamm4::gamm4()` or *brms*

`ti()` fits pure smooth interactions; where the main effects of `x` and `z` have been removed from the basis

---

# Tensor product smooths

.center[
&lt;img src="resources/wood-gams-2ed-fig-5-17-tensor-product.svg" width="50%" /&gt;
]

---

# Factor smooth interactions I

Several ways to include factor smooth interactions

1. `by` variable smooths
    * entirely separate smooth function for each level of the factor
	* each has it's own smoothness parameter
	* centred (no group means) so include factor as a fixed effect
	* `y ~ f + s(x, by = f)`
2. `by` variable smooths *ordered* variant
    * main smooth for reference level
	* smooth deviation from reference for other levels
	* each smooth has it's own smoothness parameter
	* `y ~ o + s(x) + s(x, by = o)`

---

# Factor smooth interactions II

Several ways to include factor smooth interactions

3. `bs = 'fs'` basis
    * smooth function for each level of the function
	* share a common smoothness parameter
	* fully penalized; include group means
	* closer to random effects
	* `y ~ s(x, f, bs = 'fs')`
3. `bs = 'sz'` basis (**New**)
    * provides orthogonal smooth differences from main effect
	* share a common smoothness parameter
	* include group means
	* `y ~ s(x) + s(x, f, bs = 'sz')`

---

# Random effects

When fitted with REML or ML, smooths can be viewed as just fancy random effects

Inverse is true too; random effects can be viewed as smooths

If you have simple random effects you can fit those in `gam()` and `bam()` without needing the more complex GAMM functions `gamm()` or `gamm4::gamm4()`

These two models are equivalent


```r
m_nlme &lt;- lme(travel ~ 1, data = Rail, ~ 1 | Rail, method = "REML") 

m_gam  &lt;- gam(travel ~ s(Rail, bs = "re"), data = Rail, method = "REML")
```

---

# Random effects

The random effect basis `bs = 're'` is not as computationally efficient as *nlme* or *lme4* for fitting

* complex random effects terms, or
* random effects with many levels

Instead see `gamm()` and `gamm4::gamm4()`

* `gamm()` fits using `lme()`
* `gamm4::gamm4()` fits using `lmer()` or `glmer()`

For non Gaussian models use `gamm4::gamm4()`

---
class: inverse middle center big-subsection

# gratia ðŸ“¦

---

# gratia ðŸ“¦

{gratia} is a package for working with GAMs

Mostly limited to GAMs fitted with {mgcv} &amp; {gamm4}

Follows (sort of) Tidyverse principles

* return tibbles (data frames)

* suitable for plotting with {ggplot2}

* graphics use {ggplot2} and {patchwork}

---
class: inverse center middle subsection

# Model checking

---

# Model checking

So you have a GAM:

- How do you know you have the right degrees of freedom? `gam.check()`

- Diagnosing model issues: `gam.check()` part 2

---

# GAMs are models too

How accurate your predictions depends on how good the model is

&lt;img src="index_files/figure-html/misspecify-1.svg" width="95%" /&gt;

---
class: inverse center middle subsection

# How do we test how well our model fits?

---

# Simulated data

`y` varies sinusoidally with `x1`

`y` varies sigmoidally with `x2`

Simulate response data from Gaussian, negative binomial, &amp; binomial distributions

```r
set.seed(2)
n &lt;- 400
x1 &lt;- rnorm(n)
x2 &lt;- rnorm(n)
y_val &lt;- 1 + 2*cos(pi*x1) + 2/(1+exp(-5*(x2)))
y_norm &lt;- y_val + rnorm(n, 0, 0.5)
y_negbinom &lt;- rnbinom(n, mu = exp(y_val),size=10)
y_binom &lt;- rbinom(n,1,prob = exp(y_val)/(1+exp(y_val)))
```

---

# Simulated data

![](index_files/figure-html/sims_plot-1.svg)&lt;!-- --&gt;

---
class: inverse middle center subsection

# gam.check() part 1: do you have the right functional form?

---

# How well does the model fit?

- Many choices: k, family, type of smoother, &amp;hellip;

- How do we assess how well our model fits?

---

# Basis size *k*

- Set `k` per term

- e.g. `s(x, k=10)` or `s(x, y, k=100)`

- Penalty removes "extra" wigglyness
    
	- *up to a point!*

- (But computation is slower with bigger `k`)

---

# Checking basis size


```r
norm_model_1 &lt;- gam(y_norm ~ s(x1, k = 4) + s(x2, k = 4), method = 'REML')
gam.check(norm_model_1)
```

```
## 
## Method: REML   Optimizer: outer newton
## full convergence after 8 iterations.
## Gradient range [-0.0003467788,0.0005154578]
## (score 736.9402 &amp; scale 2.252304).
## Hessian positive definite, eigenvalue range [0.000346021,198.5041].
## Model rank =  7 / 7 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k'.
## 
##         k'  edf k-index p-value    
## s(x1) 3.00 1.00    0.13  &lt;2e-16 ***
## s(x2) 3.00 2.91    1.04    0.83    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---

# Checking basis size


```r
norm_model_2 &lt;- gam(y_norm ~ s(x1, k = 12) + s(x2, k = 4), method = 'REML')
gam.check(norm_model_2)
```

```
## 
## Method: REML   Optimizer: outer newton
## full convergence after 11 iterations.
## Gradient range [-5.658609e-06,5.392657e-06]
## (score 345.3111 &amp; scale 0.2706205).
## Hessian positive definite, eigenvalue range [0.967727,198.6299].
## Model rank =  15 / 15 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k'.
## 
##          k'   edf k-index p-value   
## s(x1) 11.00 10.84    0.99    0.38   
## s(x2)  3.00  2.98    0.86    0.01 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---

# Checking basis size


```r
norm_model_3 &lt;- gam(y_norm ~ s(x1, k = 12) + s(x2, k = 12),method = 'REML')
gam.check(norm_model_3)
```

```
## 
## Method: REML   Optimizer: outer newton
## full convergence after 8 iterations.
## Gradient range [-1.136192e-08,6.794565e-13]
## (score 334.2084 &amp; scale 0.2485446).
## Hessian positive definite, eigenvalue range [2.812271,198.6868].
## Model rank =  23 / 23 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k'.
## 
##          k'   edf k-index p-value
## s(x1) 11.00 10.85    0.98    0.31
## s(x2) 11.00  7.95    0.95    0.15
```

---

# Checking basis size

![](index_files/figure-html/gam_check_norm4-1.svg)&lt;!-- --&gt;

---

# Checking basis size

Two common ways that the `k.check()` (`gam.check()`) test will reject the null hypothesis of adequate basis size

1. Unmodelled auto-correlation

2. Incorrectly specified mean-variance relationship (wrong `family`)

---

# Checking basis size (alt)

Model with `k` set to be too low for each smooth


```r
norm_model_1 &lt;- gam(y_norm ~ s(x1, k = 4) + s(x2, k = 4), method = "REML")
k.check(norm_model_1)
```

```
##       k'      edf   k-index p-value
## s(x1)  3 1.001710 0.1250378    0.00
## s(x2)  3 2.913909 1.0446399    0.85
```

Alternate way of assessing if the basis size is sufficient is to take the deviance residuals from the model and fit a GAM to those with `family = quasi(link = "identity", variance = "constant")` and `k` doubled

???

Remember we had this model where we had set `k` to be too low for each of the two smooths

---

# Checking basis size (alt)


```r
res &lt;- resid(norm_model_1, type = "deviance")

res_model &lt;- gam(res ~ s(x1, k = 12) + s(x2, k = 12),
  method = "REML",
  family = quasi(link = "identity", variance = "constant"))
edf(res_model)
```

```
## # A tibble: 2 Ã— 2
##   smooth   edf
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 s(x1)  10.8 
## 2 s(x2)   7.29
```

---

# Checking basis size (alt)

```r
draw(res_model)
```

&lt;img src="index_files/figure-html/alt-basis-dim-check-3-1.svg" width="95%" style="display: block; margin: auto;" /&gt;

---
class: inverse middle center subsection

# Model diagnostics

---
class: inverse middle center subsection

# Using gam.check() part 2: visual checks

---

# gam.check() plots

`gam.check()` creates 4 plots: 

1. Quantile-quantile plots of residuals. If the model is right, should follow 1-1 line

2. Histogram of residuals

3. Residuals vs. linear predictor

4. Observed vs. fitted values

`gam.check()` uses deviance residuals by default

---

# Gaussian data, Gaussian model


```r
norm_model &lt;- gam(y_norm ~ s(x1, k=12) + s(x2, k=12), method="REML")
gam.check(norm_model, rep = 500)
```

&lt;img src="index_files/figure-html/gam_check_plots1-1.svg" width="90%" style="display: block; margin: auto;" /&gt;

---

# Negative binomial data, Poisson model


```r
pois_model &lt;- gam(y_negbinom ~ s(x1, k=12) + s(x2, k=12), family=poisson, method="REML")
gam.check(pois_model, rep = 500)
```

&lt;img src="index_files/figure-html/gam_check_plots2-1.svg" width="90%" style="display: block; margin: auto;" /&gt;

---

# NegBin data, NegBin model


```r
negbin_model &lt;- gam(y_negbinom ~ s(x1, k=12) + s(x2, k=12), family = nb, method="REML")
gam.check(negbin_model, rep = 500)
```

&lt;img src="index_files/figure-html/gam_check_plots3-1.svg" width="90%" style="display: block; margin: auto;" /&gt;

---

# NegBin data, NegBin model


```r
appraise(negbin_model, method = 'simulate')
```

![](index_files/figure-html/appraise-gam-check-example-1.svg)&lt;!-- --&gt;

---
class: inverse middle center subsection

# Example

---

# Galveston Bay

.row[

.col-6[
Cross Validated question

&gt; I have a dataset of water temperature measurements taken from a large waterbody at irregular intervals over a period of decades. (Galveston Bay, TX if youâ€™re interested)

&lt;https://stats.stackexchange.com/q/244042/1390&gt;

]

.col-6[

.center[
&lt;img src="resources/cross-validated.png" width="1223" /&gt;
]

]
]

---

# Galveston Bay

.small[

```r
galveston &lt;- read_csv("https://bit.ly/gam-galveston") %&gt;%
    mutate(datetime = as.POSIXct(paste(DATE, TIME),
                                 format = '%m/%d/%y %H:%M', tz = "CDT"),
           STATION_ID = factor(STATION_ID),
           DoY = as.numeric(format(datetime, format = '%j')),
           ToD = as.numeric(format(datetime, format = '%H')) +
               (as.numeric(format(datetime, format = '%M')) / 60))
galveston
```

```
## # A tibble: 15,276 Ã— 13
##    STATION_ID DATE     TIME   LATITUDE LONGITUDE  YEAR MONTH   DAY SEASON
##    &lt;fct&gt;      &lt;chr&gt;    &lt;time&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 
##  1 13296      6/20/91  11:04      29.5     -94.8  1991     6    20 Summer
##  2 13296      3/17/92  09:30      29.5     -94.8  1992     3    17 Spring
##  3 13296      9/23/91  11:24      29.5     -94.8  1991     9    23 Fall  
##  4 13296      9/23/91  11:24      29.5     -94.8  1991     9    23 Fall  
##  5 13296      6/20/91  11:04      29.5     -94.8  1991     6    20 Summer
##  6 13296      12/17/91 10:15      29.5     -94.8  1991    12    17 Winter
##  7 13296      6/29/92  11:17      29.5     -94.8  1992     6    29 Summer
##  8 13305      3/24/87  11:53      29.6     -95.0  1987     3    24 Spring
##  9 13305      4/2/87   13:39      29.6     -95.0  1987     4     2 Spring
## 10 13305      8/11/87  15:25      29.6     -95.0  1987     8    11 Summer
## # â„¹ 15,266 more rows
## # â„¹ 4 more variables: MEASUREMENT &lt;dbl&gt;, datetime &lt;dttm&gt;, DoY &lt;dbl&gt;, ToD &lt;dbl&gt;
```
]

---

# Galveston Bay model description

$$
`\begin{align}
  \begin{split}
      \mathrm{E}(y_i) &amp; = \alpha + f_1(\text{ToD}_i) + f_2(\text{DoY}_i) + f_3(\text{Year}_i) + f_4(\text{x}_i, \text{y}_i) + \\
        &amp; \quad f_5(\text{DoY}_i, \text{Year}_i) + f_6(\text{x}_i, \text{y}_i, \text{ToD}_i) + \\
        &amp; \quad f_7(\text{x}_i, \text{y}_i, \text{DoY}_i) + f_8(\text{x}_i, \text{y}_i, \text{Year}_i)
  \end{split}
\end{align}`
$$

* `\(\alpha\)` is the model intercept,
* `\(f_1(\text{ToD}_i)\)` is a smooth function of time of day,
* `\(f_2(\text{DoY}_i)\)` is a smooth function of day of year ,
* `\(f_3(\text{Year}_i)\)` is a smooth function of year,
* `\(f_4(\text{x}_i, \text{y}_i)\)` is a 2D smooth of longitude and latitude,

---

# Galveston Bay model description

$$
`\begin{align}
  \begin{split}
      \mathrm{E}(y_i) &amp; = \alpha + f_1(\text{ToD}_i) + f_2(\text{DoY}_i) + f_3(\text{Year}_i) + f_4(\text{x}_i, \text{y}_i) + \\
        &amp; \quad f_5(\text{DoY}_i, \text{Year}_i) + f_6(\text{x}_i, \text{y}_i, \text{ToD}_i) + \\
        &amp; \quad f_7(\text{x}_i, \text{y}_i, \text{DoY}_i) + f_8(\text{x}_i, \text{y}_i, \text{Year}_i)
  \end{split}
\end{align}`
$$

* `\(f_5(\text{DoY}_i, \text{Year}_i)\)` is a tensor product smooth of day of year and year,
* `\(f_6(\text{x}_i, \text{y}_i, \text{ToD}_i)\)` tensor product smooth of location &amp; time of day
* `\(f_7(\text{x}_i, \text{y}_i, \text{DoY}_i)\)` tensor product smooth of location day of year&amp; 
* `\(f_8(\text{x}_i, \text{y}_i, \text{Year}_i\)` tensor product smooth of location &amp; year

---

# Galveston Bay model description

$$
`\begin{align}
  \begin{split}
      \mathrm{E}(y_i) &amp; = \alpha + f_1(\text{ToD}_i) + f_2(\text{DoY}_i) + f_3(\text{Year}_i) + f_4(\text{x}_i, \text{y}_i) + \\
        &amp; \quad f_5(\text{DoY}_i, \text{Year}_i) + f_6(\text{x}_i, \text{y}_i, \text{ToD}_i) + \\
        &amp; \quad f_7(\text{x}_i, \text{y}_i, \text{DoY}_i) + f_8(\text{x}_i, \text{y}_i, \text{Year}_i)
  \end{split}
\end{align}`
$$

Effectively, the first four smooths are the main effects of

1. time of day,
2. season,
3. long-term trend,
4. spatial variation

---

# Galveston Bay model description

$$
`\begin{align}
  \begin{split}
      \mathrm{E}(y_i) &amp; = \alpha + f_1(\text{ToD}_i) + f_2(\text{DoY}_i) + f_3(\text{Year}_i) + f_4(\text{x}_i, \text{y}_i) + \\
        &amp; \quad f_5(\text{DoY}_i, \text{Year}_i) + f_6(\text{x}_i, \text{y}_i, \text{ToD}_i) + \\
        &amp; \quad f_7(\text{x}_i, \text{y}_i, \text{DoY}_i) + f_8(\text{x}_i, \text{y}_i, \text{Year}_i)
  \end{split}
\end{align}`
$$

whilst the remaining tensor product smooths model smooth interactions between the stated covariates, which model

5. how the seasonal pattern of temperature varies over time,
6. how the time of day effect varies spatially,
7. how the seasonal effect varies spatially, and
8. how the long-term trend varies spatially

---

# Galveston Bay &amp;mdash; full model


```r
knots &lt;- list(DoY = c(0.5, 366.5))
m &lt;- bam(MEASUREMENT ~
             s(ToD, k = 10) +
             s(DoY, k = 12, bs = "cc") +
             s(YEAR, k = 30) +
             s(LONGITUDE, LATITUDE, k = 100, bs = "ds", m = c(1, 0.5)) +
             ti(DoY, YEAR, bs = c("cc", "tp"), k = c(12, 15)) +
             ti(LONGITUDE, LATITUDE, ToD, d = c(2,1), bs = c("ds", "tp"),
                m = list(c(1, 0.5), NA), k = c(20, 10)) +
             ti(LONGITUDE, LATITUDE, DoY, d = c(2,1), bs = c("ds", "cc"),
                m = list(c(1, 0.5), NA), k = c(25, 12)) +
             ti(LONGITUDE, LATITUDE, YEAR, d = c(2,1), bs = c("ds", "tp"),
                m = list(c(1, 0.5), NA), k = c(25, 15)),
         data = galveston, method = "fREML", knots = knots,
         nthreads = c(6, 1), discrete = FALSE)
```

---

# Galveston Bay &amp;mdash; simpler model


```r
m.sub &lt;- bam(MEASUREMENT ~
             s(ToD, k = 10) +
             s(DoY, k = 12, bs = "cc") +
             s(YEAR, k = 30) +
             s(LONGITUDE, LATITUDE, k = 100, bs = "ds", m = c(1, 0.5)) +
             ti(DoY, YEAR, bs = c("cc", "tp"), k = c(12, 15)),
         data = galveston, method = "fREML", knots = knots,
         nthreads = c(4, 1), discrete = TRUE)
```

---

# Galveston Bay &amp;mdash; simpler model?


```r
AIC(m, m.sub)
```

```
##             df      AIC
## m     484.7690 58544.03
## m.sub 238.2766 59193.50
```
---

# Galveston Bay &amp;mdash; simpler model?

.smaller[

```r
anova(m, m.sub, test = "F")
```

```
## Analysis of Deviance Table
## 
## Model 1: MEASUREMENT ~ s(ToD, k = 10) + s(DoY, k = 12, bs = "cc") + s(YEAR, 
##     k = 30) + s(LONGITUDE, LATITUDE, k = 100, bs = "ds", m = c(1, 
##     0.5)) + ti(DoY, YEAR, bs = c("cc", "tp"), k = c(12, 15)) + 
##     ti(LONGITUDE, LATITUDE, ToD, d = c(2, 1), bs = c("ds", "tp"), 
##         m = list(c(1, 0.5), NA), k = c(20, 10)) + ti(LONGITUDE, 
##     LATITUDE, DoY, d = c(2, 1), bs = c("ds", "cc"), m = list(c(1, 
##         0.5), NA), k = c(25, 12)) + ti(LONGITUDE, LATITUDE, YEAR, 
##     d = c(2, 1), bs = c("ds", "tp"), m = list(c(1, 0.5), NA), 
##     k = c(25, 15))
## Model 2: MEASUREMENT ~ s(ToD, k = 10) + s(DoY, k = 12, bs = "cc") + s(YEAR, 
##     k = 30) + s(LONGITUDE, LATITUDE, k = 100, bs = "ds", m = c(1, 
##     0.5)) + ti(DoY, YEAR, bs = c("cc", "tp"), k = c(12, 15))
##   Resid. Df Resid. Dev      Df Deviance      F    Pr(&gt;F)    
## 1     14684      38759                                      
## 2     15022      41769 -338.25  -3009.8 3.3979 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]

---

# Galveston Bay &amp;mdash; full model summary

.small[

```r
summary(m)
```

```
## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## MEASUREMENT ~ s(ToD, k = 10) + s(DoY, k = 12, bs = "cc") + s(YEAR, 
##     k = 30) + s(LONGITUDE, LATITUDE, k = 100, bs = "ds", m = c(1, 
##     0.5)) + ti(DoY, YEAR, bs = c("cc", "tp"), k = c(12, 15)) + 
##     ti(LONGITUDE, LATITUDE, ToD, d = c(2, 1), bs = c("ds", "tp"), 
##         m = list(c(1, 0.5), NA), k = c(20, 10)) + ti(LONGITUDE, 
##     LATITUDE, DoY, d = c(2, 1), bs = c("ds", "cc"), m = list(c(1, 
##         0.5), NA), k = c(25, 12)) + ti(LONGITUDE, LATITUDE, YEAR, 
##     d = c(2, 1), bs = c("ds", "tp"), m = list(c(1, 0.5), NA), 
##     k = c(25, 15))
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 21.61913    0.02078    1040   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Approximate significance of smooth terms:
##                                 edf Ref.df         F  p-value    
## s(ToD)                        1.000   1.00    25.980 8.79e-07 ***
## s(DoY)                        9.896  10.00 13430.960  &lt; 2e-16 ***
## s(YEAR)                      28.026  28.75    75.994  &lt; 2e-16 ***
## s(LONGITUDE,LATITUDE)        69.104  99.00     6.713  &lt; 2e-16 ***
## ti(DoY,YEAR)                131.299 140.00    34.690  &lt; 2e-16 ***
## ti(LONGITUDE,LATITUDE,ToD)   50.398 171.00     0.981  &lt; 2e-16 ***
## ti(LONGITUDE,LATITUDE,DoY)   92.991 240.00     1.301  &lt; 2e-16 ***
## ti(LONGITUDE,LATITUDE,YEAR)  91.791 287.00     1.305  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-sq.(adj) =   0.94   Deviance explained = 94.2%
## fREML =  29799  Scale est. = 2.6187    n = 15276
```
]

---

# Galveston Bay &amp;mdash; full model plot


```r
plot(m, pages = 1, scheme = 2, shade = TRUE)
```

![](index_files/figure-html/galveston-full-model-plot-1.svg)&lt;!-- --&gt;

---

# Galveston Bay &amp;mdash; full model plot


```r
draw(m, scales = "free", rug = FALSE, n = 50) +  plot_layout(widths = 1) &amp;
  theme(strip.text.x = element_text(size = 8))
```

&lt;img src="index_files/figure-html/galveston-full-model-draw-1.svg" width="90%" style="display: block; margin: auto;" /&gt;

---

# Galveston Bay &amp;mdash; predict


```r
pdata &lt;- data_slice(m, ToD = 12, DoY = 180,
                    YEAR = evenly(YEAR, by = 1),
                    LONGITUDE = evenly(LONGITUDE, n = 50),
                    LATITUDE  = evenly(LATITUDE, n = 50))
fv &lt;- fitted_values(m, data = pdata)
# set fitted values to NA for grid points that are too far from the data
ind &lt;- too_far(pdata$LONGITUDE, pdata$LATITUDE,
               galveston$LONGITUDE, galveston$LATITUDE, dist = 0.1)
fv &lt;- fv %&gt;%
  mutate(fitted = if_else(ind, NA_real_, fitted))
```

---

# Galveston Bay &amp;mdash; plot


```r
plt &lt;- ggplot(fv, aes(x = LONGITUDE, y = LATITUDE)) +
    geom_raster(aes(fill = fitted)) + facet_wrap(~ YEAR, ncol = 12) +
    scale_fill_viridis(name = expression(degree*C), option = "plasma",
      na.value = "transparent") +
    coord_quickmap() +
    scale_x_continuous(guide = guide_axis(n.dodge = 2,
                                          check.overlap = TRUE)) +
    theme(legend.position = "top")
plt
```

---

# Galveston Bay &amp;mdash; plot

![](index_files/figure-html/galveston-full-predict-plot-1.svg)&lt;!-- --&gt;

---

# Galveston Bay &amp;mdash;



.center[![](resources/galveston-animation.gif)]

---

# Galveston Bay &amp;mdash; plot trends


```r
ds &lt;- data_slice(m, ToD = 12, DoY = c(1, 90, 180, 270),
  YEAR = evenly(YEAR, n = 250),
  LONGITUDE = -94.8751, LATITUDE  = 29.50866)
fv &lt;- fitted_values(m, data = ds, scale = "response")

plt2 &lt;- ggplot(fv, aes(x = YEAR, y = fitted, group = factor(DoY))) +
    geom_ribbon(aes(ymin = lower, ymax = upper), fill = "black", alpha = 0.2) +
    geom_line() + facet_wrap(~ DoY, scales = "free_y") +
    labs(x = NULL, y = expression(Temperature ~ (degree * C)))
plt2
```

---

# Galveston Bay &amp;mdash; plot trends

![](index_files/figure-html/galveston-trends-by-month-1.svg)&lt;!-- --&gt;

---
class: inverse center middle subsection

# Example

---

# Rat hormone experiment

https://bit.ly/rat-hormone

Study on the effects of testosterone on the growth of rats (Molenberghs and Verbeke, 2000)

50 rats randomly assigned to 1 of 3 groups:

1. a control group
2. a group receiving low doses of Decapeptyl
3. a high Decapeptyl dose group

Decapeptyl inhibits the preoduction of testosterone

Experiment started (day 1) when rats were 45 days old and from day 50 the size of each rat's head was measured via an x-ray image



???

By way of an example, I'm going to use a data set from a study on the effects of testosterone on the growth of rats from Molenberghs and Verbeke (2000), which was analysed in Fahrmeir et al. (2013), from were I also obtained the data. In the experiment, 50 rats were randomly assigned to one of three groups; a control group or a group receiving low or high doses of Decapeptyl, which inhibits testosterone production. The experiment started when the rats were 45 days old and starting with the 50th day, the size of the rat's head was measured via an X-ray image. You can download the data.

---

# Rat hormone experiment

![](index_files/figure-html/plot-rat-data-1.svg)&lt;!-- --&gt;

---

# Rat hormone experiment

To linearise the `time` variable, a transformation was applied

`$$\mathtt{transf\_time} = \log (1 + (\mathtt{time} - 45) / 10)$$`

The number of observations per rat is very variable


```
## # A tibble: 7 Ã— 2
##       n n_rats
##   &lt;int&gt;  &lt;int&gt;
## 1     1      4
## 2     2      3
## 3     3      5
## 4     4      9
## 5     5      5
## 6     6      2
## 7     7     22
```

Only 22 of the 50 rats have the complete 7 measurements by day 110

---
class: inverse center middle subsection

# HGAMs

---

# Hierarchical models

The general term encompassing

* Random effects
* Mixed effects
* Mixed models
* &amp;hellip;

Models are *hierarchical* because we have effects on the response at different scales

Data are grouped in some way

---

# Hierarchical GAMs

Hierarchical GAMs or HGAMs are what we (Pedersen et al 2019 *PeerJ*) called the marriage of

1. Hierarchical GLMs (aka GLMMs, aka Hierarchical models)
2. GAMs

Call them HGAMs if you want but these are really just *hierarchical models*

There's nothing special HGAMs once we've created the basis functions

---

# Hierarchical GAMs

Pedersen et al (2019) *PeerJ* described 6 models

&lt;img src="resources/lawton-et-al-hgam-locust-paper-fig.svg" width="95%" style="display: block; margin: auto;" /&gt;

.small[Source: [Lawton *et al* (2022) *Ecography*](http://doi.org/10.1111/ecog.05763) modified from [Pedersen *et al* (2019) *PeerJ*](http://doi.org/10.7717/peerj.6876)]

---

# Global effects

What we called *global effects* or *global trends* are a bit like population-level effects in mixed-model speak

They aren't quite, but they are pretty close to the average smooth effect over all the data

Really these are *group-level effects* or *group-level effects* where data has multiple levels

1. "population", top level grouping (i.e. everything)
2. treatment level,
3. etc

---

# Subject-specific effects

Within these groups we have *subject-specific effects* &amp;mdash; which could be smooth

Repeated observations on a set of subjects over time say

Those subjects may be within groups (treatment groups say)

We may or may not have group-level (*global*; treatment) effects

---

# Hierarchical GAMs

These models are just different ways to decompose the data

If there are common (non-linear) effects that explain variation for all subjects in a group it may be more parsimonious to

* model those common effects plus subject-specific differences, instead of

* modelling each subject-specific response individually

---
class: inverse center middle subsection

# Example

---

# Next steps

Read Simon Wood's book!

Noam Ross' free GAM Course

&lt;https://noamross.github.io/gams-in-r-course/&gt;

Lots more material on my [GAM course](https://github.com/gavinsimpson/physalia-gam-course) for [Physalia Courses](https://www.physalia-courses.org/)

A couple of papers:

.smaller[
1. Simpson, G.L., 2018. Modelling Palaeoecological Time Series Using Generalised Additive Models. Frontiers in Ecology and Evolution 6, 149. https://doi.org/10.3389/fevo.2018.00149
2. Pedersen, E.J., Miller, D.L., Simpson, G.L., Ross, N., 2019. Hierarchical generalized additive models in ecology: an introduction with mgcv. PeerJ 7, e6876. https://doi.org/10.7717/peerj.6876
]

Also see my blog: [www.fromthebottomoftheheap.net](http://www.fromthebottomoftheheap.net)

---

# Acknowledgments

### mgcv &amp; related theory

* Simon Wood
* Matteo Fasiolo

### Fellow GAM colleagues

* David Miller
* Eric Pedersen
* Noam Ross

### Slides

* HTML Slide deck [bit.ly/ukraine-gam-slides](https://bit.ly/ukraine-gam-slides) &amp;copy; Simpson (2023) [![Creative Commons Licence](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)
* RMarkdown [bit.ly/ukraine-gam-repo](https://bit.ly/ukraine-gam-repo)
* DOI: [10.5281/zenodo.7825960](https://doi.org/10.5281/zenodo.7825960)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
